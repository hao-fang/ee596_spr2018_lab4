{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Recurrent Neural Network Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Unit\n",
    "Let's define the following recurrent unit\n",
    "$$ h_t = f(W_{hh} h_{t-1} + W_{hx}x_t) $$\n",
    "where $$t=1,\\ldots,\\infty,$$\n",
    "$$h_t\\in\\mathbb{R}^{n\\times 1} \\text{ is the current state}, $$\n",
    "$$h_{t-1}\\in\\mathbb{R}^{n\\times 1} \\text{ is the previous state}, $$\n",
    "$$x_t \\in\\mathbb{R}^{d\\times 1} \\text{ is the current input}, $$\n",
    "$$W_{hh}\\in\\mathbb{R}^{n\\times n}, W_{hx} \\in\\mathbb{R}^{n\\times d} \\text{ are parameter matrices}.$$\n",
    "Here, f(y) is the sigmoid function, $$f(y)=\\frac{1}{1+\\exp(-y)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the sigmoid function look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Plot sigmoid function in 1D\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(y):\n",
    "    return 1 / (1 + np.exp(-y))\n",
    "\n",
    "# You can change the bound from 5, 10, 20 up to 50 to see the shape change\n",
    "bound = 5\n",
    "x = np.arange(-bound, bound, 0.5)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## How to code algebracially?\n",
    "\n",
    "## Matrix-vector multiplication\n",
    "## A is a n-by-d maxtrix, b is a dx1 vector\n",
    "## c is a n-by-1 vector, the multiplication result of A and b\n",
    "n = 3\n",
    "d = 5\n",
    "A = np.ones([n, d])   # initialize A as a n-by-d all-one matrix\n",
    "print 'The size of matrix A is {}-by-{}'.format(A.shape[0], A.shape[1])\n",
    "b = np.ones([d, 1]) # initialize b as a d-by-1 all-one vector\n",
    "print 'The size of vector b is {}-by-{}'.format(b.shape[0], b.shape[1])\n",
    "c = np.dot(A, b)\n",
    "print 'The size of vector c is {}-by-{}'.format(c.shape[0], c.shape[1])\n",
    "\n",
    "print 'The vector c is'\n",
    "print c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Function\n",
    "Let's code up the forward function of the recurrent unit.\n",
    "Please fill in < input your code here > below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Given input x, previous state hprev, parameter matrices Whx and Whh\n",
    "## Compute the current state h\n",
    "## Code up the forward function of the recurrent unit define above\n",
    "## Remember to return the current state h\n",
    "\n",
    "hidden_size = 10\n",
    "input_size = 5\n",
    "\n",
    "Whh = np.zeros([hidden_size, hidden_size])\n",
    "Whh += np.random.uniform(-0.1, 0.1, [hidden_size, hidden_size])\n",
    "\n",
    "Whx = np.zeros([hidden_size, input_size])\n",
    "Whx += np.random.uniform(-0.1, 0.1, [hidden_size, input_size])\n",
    "\n",
    "def forward_function(x, hprev, Whx, Whh):\n",
    "    ## first compute the matrix-vector multiplication\n",
    "    # Whx x + Whh hprev\n",
    "    h = <input your code here>\n",
    "    ## use the sigmoid function to compute the current state\n",
    "    h = sigmoid(<input your code here>)\n",
    "    return h\n",
    "\n",
    "\n",
    "x = np.random.randn(input_size, 1)\n",
    "hprev = np.random.randn(hidden_size, 1)\n",
    "h = forward_function(x, hprev, Whx, Whh)\n",
    "\n",
    "print 'Successfully forwarded!'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## test whether the forward function is correct\n",
    "x_test = np.zeros([input_size, 1])\n",
    "hprev_test = np.zeros([hidden_size, 1])\n",
    "h_test = forward_function(x_test, hprev_test, Whx, Whh)\n",
    "assert np.all(h_test - 0.5 < 1e-7)\n",
    "print 'Test passed!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Function\n",
    "An important component of finding the best parameters is to train it through optimization. In order to do that, we first have to learn to take the derivative of the above unit.\n",
    "Let's do the derivatives in the matrix form!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Here is the cheatsheet:\n",
    "\n",
    "First let us define\n",
    "$$ y = W_{hh} h_{t-1} + W_{hx} x_t.$$\n",
    "Then\n",
    "$$ h_t = f(y) $$.\n",
    "#### Important gradient one: \n",
    "\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} = W_{hh}^T f\\prime(y),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f\\prime(y)=\\frac{\\partial f(y)}{y}=\\frac{\\partial \\frac{1}{1+\\exp(-y)}}{\\partial y}=\\frac{\\exp(-y)}{(1+\\exp(-y))^2} = (1-f(y))f(y),\n",
    "$$ is the gradient of sigmoid function.\n",
    "Can you verfiy the above gradient?\n",
    "\n",
    "#### Important gradient two:\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial W_{hh}} = \\frac{\\partial h_t}{\\partial y} h_{t-1}^T = f\\prime(y) h_{t-1}^T\n",
    "$$\n",
    "\n",
    "#### Important gradient three:\n",
    "$$ \n",
    "\\frac{\\partial h_t}{\\partial W_{hx}} = \\frac{\\partial h_t}{\\partial y} x_{t}^T = f\\prime(y) x_{t}^T\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code up a backward function which accepts a training/error signal to weight the gradients and output the three gradients above. \n",
    "Please fill in < input your code here > below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Given a training/error signal dEdh, input x, previous state hprev\n",
    "## parameters Whh, Whx\n",
    "## Return the three graidents above\n",
    "\n",
    "def backward_function(x, hprev, dEdh, Whx, Whh):\n",
    "    ## compute the gradient of sigmoid function\n",
    "    f_prime = <input your code here>\n",
    "    ## weight the gradient by training/error signal\n",
    "    f_prime *= dEdh\n",
    "    ## compute the gradient one \n",
    "    dEdhprev = <input your code here>\n",
    "    ## compute the gradient two\n",
    "    dWhh = <input your code here>\n",
    "    ## compute the gradient three\n",
    "    dWhx = <input your code here>\n",
    "    return f_prime.T, dWhx, dWhh\n",
    "\n",
    "# compute gradients\n",
    "E = np.sum(h)\n",
    "dEdh = np.ones([hidden_size, 1])\n",
    "\n",
    "dEdhprev, dWhx, dWhh = backward_function(x, hprev, dEdh, Whx, Whh)\n",
    "\n",
    "print 'Successfully backpropgated!'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Check whether your code is right\n",
    "\n",
    "# Numerical gradient computation\n",
    "epsilon = 1e-7                                         \n",
    "numdWhh = np.zeros([hidden_size,hidden_size])       \n",
    "for i in range(hidden_size):                           \n",
    "    for j in range(hidden_size):                       \n",
    "        newWhh = np.copy(Whh)                          \n",
    "        newWhh[i,j] += epsilon                         \n",
    "                                                           \n",
    "        h = forward_function(x, hprev, Whx, newWhh)\n",
    "        newE = np.sum(h)                               \n",
    "        numdWhh[i,j] = (newE - E) / epsilon            \n",
    "                                                           \n",
    "diff = np.sum(numdWhh - dWhh)   \n",
    "print 'Diff is ', diff\n",
    "assert diff < 1e-3                                     \n",
    "print 'Test Passed!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have learned the key part of a recurrent neural network.\n",
    "Please copy the forward_function and backward_function to rnn_unit.py.\n",
    "Note there are some minor naming changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
